{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [0]\n",
      "X_test.shape = (200, 91)\n",
      "blend_train.shape = (800, 3)\n",
      "blend_test.shape = (200, 3)\n",
      "Training classifier [0]\n",
      "Fold [0]\n",
      "Fold [1]\n",
      "Fold [2]\n",
      "Fold [3]\n",
      "Fold [4]\n",
      "Training classifier [1]\n",
      "Fold [0]\n",
      "Fold [1]\n",
      "Fold [2]\n",
      "Fold [3]\n",
      "Fold [4]\n",
      "Training classifier [2]\n",
      "Fold [0]\n",
      "Fold [1]\n",
      "Fold [2]\n",
      "Fold [3]\n",
      "Fold [4]\n",
      "Y_dev.shape = 800\n",
      "Accuracy = 0.86\n",
      "\n",
      "Best score = 0.86\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Purpose: This script tries to implement a technique called stacking/blending/stacked generalization.\n",
    "The reason I have to make this a runnable script because I found that there isn't really any\n",
    "readable code that demonstrates this technique. You may find the pseudocode in various papers but they\n",
    "are all each kind of different.\n",
    "\n",
    "Author: Eric Chio \"log0\" <im.ckieric@gmail.com>\n",
    "\n",
    "======================================================================================================\n",
    "Summary:\n",
    "\n",
    "Just to test an implementation of stacking. Using a cross-validated random forest and SVMs, I was\n",
    "only able to achieve an accuracy of about 88% (with 1000 trees and up). Using stacked generalization \n",
    "I have seen a maximum of 93.5% accuracy. It does take runs to find it out though. This uses only \n",
    "(10, 20, 10) trees for the three classifiers.\n",
    "\n",
    "This code is heavily inspired from the code shared by Emanuele (https://github.com/emanuele) , but I\n",
    "have cleaned it up to makeit available for easy download and execution.\n",
    "\n",
    "======================================================================================================\n",
    "Methodology:\n",
    "\n",
    "Three classifiers (RandomForestClassifier, ExtraTreesClassifier and a GradientBoostingClassifier\n",
    "are built to be stacked by a LogisticRegression in the end.\n",
    "\n",
    "Some terminologies first, since everyone has their own, I'll define mine to be clear:\n",
    "- DEV SET, this is to be split into the training and validation data. It will be cross-validated.\n",
    "- TEST SET, this is the unseen data to validate the generalization error of our final classifier. This\n",
    "set will never be used to train.\n",
    "\n",
    "======================================================================================================\n",
    "Log Output:\n",
    "\n",
    "X_test.shape = (62L, 6L)\n",
    "blend_train.shape = (247L, 3L)\n",
    "blend_test.shape = (62L, 3L)\n",
    "Training classifier [0]\n",
    "Fold [0]\n",
    "Fold [1]\n",
    "Fold [2]\n",
    "Fold [3]\n",
    "Fold [4]\n",
    "Training classifier [1]\n",
    "Fold [0]\n",
    "Fold [1]\n",
    "Fold [2]\n",
    "Fold [3]\n",
    "Fold [4]\n",
    "Training classifier [2]\n",
    "Fold [0]\n",
    "Fold [1]\n",
    "Fold [2]\n",
    "Fold [3]\n",
    "Fold [4]\n",
    "Y_dev.shape = 247\n",
    "Accuracy = 0.935483870968\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "def run(X, Y):\n",
    "    # We need to transform the string output to numeric\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(Y)\n",
    "    Y = label_encoder.transform(Y)\n",
    "    \n",
    "    # The DEV SET will be used for all training and validation purposes\n",
    "    # The TEST SET will never be used for training, it is the unseen set.\n",
    "    dev_cutoff = len(Y) * 4/5\n",
    "    X_dev = X[:dev_cutoff]\n",
    "    Y_dev = Y[:dev_cutoff]\n",
    "    X_test = X[dev_cutoff:]\n",
    "    Y_test = Y[dev_cutoff:]\n",
    "    \n",
    "    n_trees = 10\n",
    "    n_folds = 5\n",
    "    \n",
    "    # Our level 0 classifiers\n",
    "    clfs = [\n",
    "        RandomForestClassifier(n_estimators = n_trees, criterion = 'gini'),\n",
    "        ExtraTreesClassifier(n_estimators = n_trees * 2, criterion = 'gini'),\n",
    "        GradientBoostingClassifier(n_estimators = n_trees),\n",
    "    ]\n",
    "    \n",
    "    # Ready for cross validation\n",
    "    skf = list(StratifiedKFold(Y_dev, n_folds))\n",
    "    \n",
    "    # Pre-allocate the data\n",
    "    blend_train = np.zeros((X_dev.shape[0], len(clfs))) # Number of training data x Number of classifiers\n",
    "    blend_test = np.zeros((X_test.shape[0], len(clfs))) # Number of testing data x Number of classifiers\n",
    "    \n",
    "    print 'X_test.shape = %s' % (str(X_test.shape))\n",
    "    print 'blend_train.shape = %s' % (str(blend_train.shape))\n",
    "    print 'blend_test.shape = %s' % (str(blend_test.shape))\n",
    "    \n",
    "    # For each classifier, we train the number of fold times (=len(skf))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print 'Training classifier [%s]' % (j)\n",
    "        blend_test_j = np.zeros((X_test.shape[0], len(skf))) # Number of testing data x Number of folds , we will take the mean of the predictions later\n",
    "        for i, (train_index, cv_index) in enumerate(skf):\n",
    "            print 'Fold [%s]' % (i)\n",
    "            \n",
    "            # This is the training and validation set\n",
    "            X_train = X_dev[train_index]\n",
    "            Y_train = Y_dev[train_index]\n",
    "            X_cv = X_dev[cv_index]\n",
    "            Y_cv = Y_dev[cv_index]\n",
    "            \n",
    "            clf.fit(X_train, Y_train)\n",
    "            \n",
    "            # This output will be the basis for our blended classifier to train against,\n",
    "            # which is also the output of our classifiers\n",
    "            blend_train[cv_index, j] = clf.predict(X_cv)\n",
    "            blend_test_j[:, i] = clf.predict(X_test)\n",
    "        # Take the mean of the predictions of the cross validation set\n",
    "        blend_test[:, j] = blend_test_j.mean(1)\n",
    "    \n",
    "    print 'Y_dev.shape = %s' % (Y_dev.shape)\n",
    "    \n",
    "    # Start blending!\n",
    "    bclf = LogisticRegression()\n",
    "    bclf.fit(blend_train, Y_dev)\n",
    "    \n",
    "    # Predict now\n",
    "    Y_test_predict = bclf.predict(blend_test)\n",
    "    score = metrics.accuracy_score(Y_test, Y_test_predict)\n",
    "    print 'Accuracy = %s' % (score)\n",
    "    \n",
    "    return score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    best_score = 0.0\n",
    "        # Note: file is comma-delimited\n",
    "    X = np.genfromtxt(\"Data/kaggle.X1.train.txt\",delimiter=',')\n",
    "    Y = np.genfromtxt(\"Data/kaggle.Y.train.txt\",delimiter=',')\n",
    "    \n",
    "    X = X[:1000,:]\n",
    "    Y = Y[:1000]\n",
    "    Y = np.array(Y > 0, dtype = 'int')\n",
    "    # run many times to get a better result, it's not quite stable.\n",
    "    for i in xrange(1):\n",
    "        print 'Iteration [%s]' % (i)\n",
    "        score = run(X, Y)\n",
    "        best_score = max(best_score, score)\n",
    "        print\n",
    "        \n",
    "    print 'Best score = %s' % (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
